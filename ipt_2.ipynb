{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lxml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SAP data from 15 December 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_data=pd.read_csv('data/SAP_15Dec21.csv', low_memory=False)\n",
    "sap_data['BaseDate'] = pd.to_datetime(sap_data['BaseDate'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918658"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((sap_data['BaseHour'] == 12) & (sap_data['ActivityType'] == '3DESPATCH'))\n",
    "temp1 = sap_data[mask]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse lookup table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with a manually prepared table mapping D365 warehouses to the 88 SAP warehouses in the 15 Dec 21 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAP_to_D365_warehouse=pd.read_csv('data/raw_data/warehouses/SAP-to-D365 warehouse mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAP_to_D365_warehouse.drop(columns={'D365_Del_Loc', 'WAREHOUSELOCATIONID', 'site_id'}, inplace=True, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with IPT_Site_Warehouse_locations to get SITE, WAREHOUSELOCATIONID, Cost Centre Financial Dimension and Financial Dimension for Stock Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPT_Site_Warehouse_locations=pd.read_csv('data/raw_data/warehouses/IPT_Site_Warehouse_locations_20221226.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses = pd.merge(\n",
    "    SAP_to_D365_warehouse,\n",
    "    IPT_Site_Warehouse_locations,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='D365_WH_NO',\n",
    "    right_on='WAREHOUSEID',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.drop(columns={'WAREHOUSEID'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.rename(columns={'Cost Centre Financial Dimension': 'Cost_Centre', 'Financial Dimension for Stock Journal':'Financial_Dimension'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with CE warehouses to get the CE Delivery Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_IPT_Active_Warehouses=pd.read_excel('data/raw_data/warehouses/CE_IPT_Active Warehouses _DeliveryLocations_20221223.xlsx',sheet_name='CE Active Warehouses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses = pd.merge(\n",
    "    warehouses,\n",
    "    CE_IPT_Active_Warehouses,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='D365_WH_NO',\n",
    "    right_on='Warehouse Id',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.drop(columns={'Warehouse Id','Description'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.rename(columns={'Delivery Location Description': 'D365_Del_Loc','SITE': 'site_id'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change notes: 8 Jan\n",
    "The following fields were in the warehouses dataframe before making these changes\n",
    "    SAP_WH_NAME,D365_WH_NO,D365_WH_NAME,D365_Del_Loc,site_id,WAREHOUSELOCATIONID\n",
    "\n",
    "The following fields were added (to be used in the journal creation):\n",
    "    Cost_Centre, Financial_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88 entries, 0 to 87\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   SAP_WH_NAME          88 non-null     object\n",
      " 1   D365_WH_NAME         88 non-null     object\n",
      " 2   D365_WH_NO           88 non-null     object\n",
      " 3   site_id              88 non-null     object\n",
      " 4   WAREHOUSELOCATIONID  88 non-null     object\n",
      " 5   Cost_Centre          88 non-null     object\n",
      " 6   Financial_Dimension  88 non-null     object\n",
      " 7   D365_Del_Loc         88 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "warehouses.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "items=pd.read_csv('data/matched_items.csv')\n",
    "items.rename(columns={'ITEMNUMBER': 'D365_ItemNo'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customers\n",
    "- Use the Bloem customers list that Andre provided.\n",
    "- Correlate with CE customers that are Outlets. Use CE_IPT_SoftDrinkOutlets_20200104.xlsx for this.\n",
    "- Include only direct customers.  Get this from the customer master.\n",
    "- Inner join this with the Bloem customers to get a list of Bloem customers (that we know will work), that also meet the criteria above.\n",
    "- Do a random match for each valid SAP_15Dec21 customer to a Bloem customer.  There will be more than one SAP customer that will be mapped to a Bloem customer.\n",
    "- We do not use Mode of Delivery from the customer master.\n",
    "-'Source Channel' field to be defaulted to 'Voice In'\n",
    "-'Order Category' field to be defaulted to 'Sales Local'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the D365 customer master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers=pd.read_csv('data/customer_master.csv', low_memory=False)\n",
    "# customers_short is just a copy of customers, without most of the columns\n",
    "customers_short = customers[['ADDRESSZIPCODE','CUSTOMERACCOUNT','ORGANIZATIONNAME', 'NAMEALIAS', 'CCBCUSTOMERTYPE']].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the address zip code for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_short['ADDRESSZIPCODE'] = customers_short['ADDRESSZIPCODE'].fillna(0)\n",
    "customers_short['ADDRESSZIPCODE'] = customers_short['ADDRESSZIPCODE'].astype(int)\n",
    "customers_short['ADDRESSZIPCODE'] = customers_short['ADDRESSZIPCODE'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the list of D365 Pilot customers (Bloem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_customers=pd.read_csv('data/Pilot Customer Accounts.csv')\n",
    "pilot_customers['Account Number']=pilot_customers['Account Number'].astype(str)\n",
    "pilot_customers.drop(columns={'Customer Category', 'Inv-Postal Code', 'Inv-Province', 'Inv-Suburb', 'Inv-City', 'Inv-Address 1', 'Inv-Address 2', 'BillTo LocationId'}, inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_customers['Account Number'] = pilot_customers['Account Number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_customers.drop(columns={'Account Name'}, inplace=True, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now import CE customers (provided by Gary on 7 Jan), and join that with pilot_customers to arrive at the list pf usable customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_customers = pd.read_excel('data/PilotAccounts.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_customers = outlet_customers.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_customers.rename(columns={'Account Name': 'D365_Account_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_customers = pd.merge(\n",
    "    outlet_customers,\n",
    "    pilot_customers,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='Account Number',\n",
    "    right_on='Account Number',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change made on 9 Jan 2023\n",
    "\n",
    "Change the “Source Channel” based on the following rules \n",
    "- Set to B2B if the “Bill I” column in Tommys data is B2B \n",
    "- Set to SFA if the “Bill I” column in Tommys data is HHT \n",
    "- Set to Voice in for all other rows \n",
    "\n",
    "The B2B order lines can only be linked to a B2B customer and the SFA and Voice In order lines should be linked to non B2B customers. \n",
    "- Gary has provided a new list of pilot accounts, with the Place Rule Columns (B2B or Blank).  This list also excludes customers with very low credit limits, which has cause submission holds on some on the FnO Prep Data.\n",
    "- These changes only need to be made for the 1ORDERCREATION order lines/files \n",
    "- This will mean we can split the order creation between the 3 different channels/test cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_data['Source Channel'] = 'Voice in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((sap_data['Bill I'] == 'B2B') & (sap_data['ActivityType'] == '1ORDERCREATION'))\n",
    "sap_data['Source Channel'].mask(mask,'B2B',inplace=True)\n",
    "mask = ((sap_data['Bill I'] == 'HHT') & (sap_data['ActivityType'] == '1ORDERCREATION'))\n",
    "sap_data['Source Channel'].mask(mask,'SFA',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_data['Order Category'] = 'Sales Local'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section creates a list of customers, padding the SAP customers with a repeating list of allowable D365 customers\n",
    "- For B2B customers, make sure they are aligned with those SAP customers for 1ORDERCREATION activity type\n",
    "- First map the D365 B2B customers (CIC Order Placement Rule == B2B) to the unique list of SAP customers where Activity Type == 1ORDERCREATION\n",
    "- Then extract the list of unique SAP customers for all lines where Activity Type != 1ORDERCREATION, remove those that have already been mapped, and map the rest to D365 customers that are non-B2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a list of unique customers from the SAP data, where 'Source Channel' == 'B2B'\n",
    "mask = (sap_data['Source Channel'] == 'B2B') \n",
    "try1 = sap_data[mask]\n",
    "unique_SAP_customers_np = try1['Customer Name'].unique()\n",
    "unique_SAP_customers1 = pd.DataFrame(unique_SAP_customers_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_SAP_customers1.rename(columns={0: 'SAP_Cust_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract  list of allowable pilot customers, where CIC Order Placement Rule = B2B\n",
    "mask = (pilot_customers['CIC Order Placement Rule'] == 'B2B') \n",
    "try1 = pilot_customers[mask]\n",
    "allowable_pilot_customers_np = try1['Account Number'].unique()\n",
    "allowable_pilot_customers = pd.DataFrame(allowable_pilot_customers_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = int(len(unique_SAP_customers_np)/len(allowable_pilot_customers_np) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = allowable_pilot_customers_np.repeat(repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_rows = len(new_array) - len(unique_SAP_customers_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a[:-n, :]\n",
    "trimmed_array = new_array[:-trim_rows :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_SAP_customers1['D365_Cust_No'] = trimmed_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the process, but now with the rest of the activity types, mapping to the remainder of the D365 customers\n",
    "\n",
    "# Extract a list of unique customers from the SAP data, where 'Source Channel' != 'B2B'\n",
    "mask = (sap_data['Source Channel'] != 'B2B')\n",
    "try1 = sap_data[mask]\n",
    "unique_SAP_customers_np = try1['Customer Name'].unique()\n",
    "unique_SAP_customers2 = pd.DataFrame(unique_SAP_customers_np)\n",
    "\n",
    "unique_SAP_customers2.rename(columns={0: 'SAP_Cust_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now exclude those SAP customers that have already been mapped to a D365 customer.  To do that, join the two dataframes\n",
    "unique_SAP_customers2 = pd.merge(\n",
    "    unique_SAP_customers2,\n",
    "    unique_SAP_customers1,\n",
    "    how=\"left\",\n",
    "    on=None,\n",
    "    left_on='SAP_Cust_Name',\n",
    "    right_on='SAP_Cust_Name',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (unique_SAP_customers2['D365_Cust_No'].isna())\n",
    "unique_SAP_customers2 = unique_SAP_customers2[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract  list of allowable pilot customers, where CIC Order Placement Rule != B2B\n",
    "mask = (pilot_customers['CIC Order Placement Rule'] != 'B2B') \n",
    "try1 = pilot_customers[mask]\n",
    "allowable_pilot_customers_np = try1['Account Number'].unique()\n",
    "allowable_pilot_customers2 = pd.DataFrame(allowable_pilot_customers_np)\n",
    "\n",
    "repeats = int(len(unique_SAP_customers2)/len(allowable_pilot_customers_np) + 1)\n",
    "new_array = allowable_pilot_customers_np.repeat(repeats)\n",
    "trim_rows = len(new_array) - len(unique_SAP_customers2)\n",
    "trimmed_array = new_array[:-trim_rows :]\n",
    "unique_SAP_customers2['D365_Cust_No'] = trimmed_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate unique_SAP_customers1 and unique_SAP_customers2\n",
    "unique_SAP_customers = pd.concat([unique_SAP_customers1, unique_SAP_customers2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23687"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sap_data['Customer Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to pull in the rest of the columns in \"pilot_customers\"\n",
    "unique_SAP_customers = pd.merge(\n",
    "    unique_SAP_customers,\n",
    "    pilot_customers,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='D365_Cust_No',\n",
    "    right_on='Account Number',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_SAP_customers.drop(columns={'Account Number'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'B2B'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_SAP_customers['CIC Order Placement Rule'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join SAP_15Dec data with D365 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for the SAP data\n",
    "sap_data_orders = sap_data[['ActivityType', 'Order Number', 'Plant_WAREHOUSE_NAME', 'Customer N', 'Customer Name', 'Material No', 'Material Description', 'BaseDate', 'BaseHour', 'Cases','Bill I','Source Channel','Order Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only filter out type 'AP'\n",
    "mask = (sap_data_orders['ActivityType'] != 'AP')\n",
    "sap_data_orders = sap_data_orders[mask]\n",
    "sap_data_orders.rename(columns={'Plant (WAREHOUSE NAME)': 'SAP_WH', 'Material No': 'SAP_MatlNo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code for updating \"Cases\".  Where Cases==0, assign a fixed quantity of 10.  Where Cases<0, negate the quantity.\n",
    "mask = (sap_data_orders['Cases'] == 0) \n",
    "sap_data_orders['Cases'].mask(mask,10,inplace=True)\n",
    "\n",
    "mask = (sap_data_orders['Cases'] < 0) \n",
    "sap_data_orders['Cases'].mask(mask,-sap_data_orders['Cases'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(\n",
    "    sap_data_orders,\n",
    "    items,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='Material Description',\n",
    "    right_on='Material_Description',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(columns={'Material Description', 'SAP_MatlNo'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2PLAN', '3DESPATCH', '4SETTLE', '1ORDERCREATION'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['ActivityType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['MOD'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now default Mode of Delivery accourding to ActivityType\n",
    "mask = (df3['ActivityType'] == '1ORDERCREATION') \n",
    "df3['MOD'].mask(mask,'01',inplace=True)\n",
    "mask = (df3['ActivityType'] == '2PLAN') \n",
    "df3['MOD'].mask(mask,'02',inplace=True)\n",
    "mask = (df3['ActivityType'] == '3DESPATCH') \n",
    "df3['MOD'].mask(mask,'03',inplace=True)\n",
    "mask = (df3['ActivityType'] == '4SETTLE') \n",
    "df3['MOD'].mask(mask,'04',inplace=True)\n",
    "mask = (df3['ActivityType'] == '5TRADERETURNS') \n",
    "df3['MOD'].mask(mask,'05',inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join SAP data with Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.merge(\n",
    "    df3,\n",
    "    warehouses,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='Plant_WAREHOUSE_NAME',\n",
    "    right_on='SAP_WH_NAME',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns={'SAP_WH_NAME', 'Plant_WAREHOUSE_NAME', 'SAP_WH_NAME', 'D365_WH_NO', 'D365_WH_NAME'}, inplace=True, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SAP data with Customer lookup table\n",
    "Note that there is no logic in replacing SAP customers with D365 customers, as only Bloem customers are available for the performance test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.merge(\n",
    "    df4,\n",
    "    unique_SAP_customers,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='Customer Name',\n",
    "    right_on='SAP_Cust_Name',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop(columns={'Customer N','Customer Name','SAP_Cust_Name'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['Order Number'] = df5['Order Number'].astype(float)\n",
    "df5['Order Number'] = df5['Order Number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop_duplicates(subset=['ActivityType', 'Order Number', 'D365_ItemNo'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop(columns={'Cost_Centre', 'Financial_Dimension'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ZRCR', 'ZFBO', 'ZFBR', 'HHT', nan, 'ZFRO', 'ZPLS', 'ZCOR', 'CRM',\n",
       "       'ZTRD', 'ZTRR', 'ZUBR', 'ZPLR', 'ZCOS', 'ZFBF', 'ZFRE', 'CRMW',\n",
       "       'ZUBS', 'ZRCS', 'ZLC', 'B2B', 'ZMTC'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5['Bill I'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CSV for the entire SAP data set\n",
    "df5.to_csv('output/15Dec_D365_orders.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop lines with negative order quantities, except where ActivityType = 5TRADERETURNS\n",
    "#mask = ((df5['Cases'] > 0) | (df5['ActivityType'] == '5TRADERETURNS'))\n",
    "#df5 = df5[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_count = pd.DataFrame(columns=['Activity type','Hour','Sales orders', 'Order lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CSVs per activity type, and for the 13th and 20th hours\n",
    "# 1ORDERCREATION\n",
    "mask = ((df5['ActivityType'] == '1ORDERCREATION') &  (df5['BaseHour'] == 12))\n",
    "peak_order_hour = df5[mask]\n",
    "peak_order_hour.to_csv('output/15Dec_D365_orders_1ORDERCREATION_12h.csv',index=False)\n",
    "\n",
    "mask = ((df5['ActivityType'] == '1ORDERCREATION') &  (df5['BaseHour'] == 19))\n",
    "peak_settlement_hour = df5[mask]\n",
    "peak_settlement_hour.to_csv('output/15Dec_D365_orders_1ORDERCREATION_19h.csv',index=False)\n",
    "\n",
    "# 2PLAN\n",
    "mask = ((df5['ActivityType'] == '2PLAN') &  (df5['BaseHour'] == 12))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_2PLAN_12h.csv',index=False)\n",
    "new_row = {'Activity type': '2PLAN', 'Hour': '12', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "mask = ((df5['ActivityType'] == '2PLAN') &  (df5['BaseHour'] == 19))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_2PLAN_19h.csv',index=False)\n",
    "new_row = {'Activity type': '2PLAN', 'Hour': '19', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "# 3DESPATCH\n",
    "mask = ((df5['ActivityType'] == '3DESPATCH') &  (df5['BaseHour'] == 12))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_3DESPATCH_12h.csv',index=False)\n",
    "new_row = {'Activity type': '3DESPATCH', 'Hour': '12', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "mask = ((df5['ActivityType'] == '3DESPATCH') &  (df5['BaseHour'] == 19))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_3DESPATCH_19h.csv',index=False)\n",
    "new_row = {'Activity type': '3DESPATCH', 'Hour': '19', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "# 4SETTLE\n",
    "mask = ((df5['ActivityType'] == '4SETTLE') &  (df5['BaseHour'] == 12))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_4SETTLE_12h.csv',index=False)\n",
    "new_row = {'Activity type': '4SETTLE', 'Hour': '12', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "mask = ((df5['ActivityType'] == '4SETTLE') &  (df5['BaseHour'] == 19))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_4SETTLE_19h.csv',index=False)\n",
    "new_row = {'Activity type': '4SETTLE', 'Hour': '19', 'Sales orders': len(df5_1['Order Number'].unique()), 'Order lines': len(df5_1)}\n",
    "rec_count = pd.concat([rec_count, pd.DataFrame([new_row])])\n",
    "\n",
    "\n",
    "# 5TRADERETURNS\n",
    "mask = ((df5['ActivityType'] == '5TRADERETURNS') &  (df5['BaseHour'] == 12))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_5TRADERETURNS_12h.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "mask = ((df5['ActivityType'] == '5TRADERETURNS') &  (df5['BaseHour'] == 19))\n",
    "df5_1 = df5[mask].copy()\n",
    "df5_1.drop(columns={'CIC Order Placement Rule'}, inplace=True, axis=1)\n",
    "df5_1.to_csv('output/15Dec_D365_orders_5TRADERETURNS_19h.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = peak_order_hour.groupby(['Source Channel']).agg({'Order Number': 'nunique','D365_ItemNo': 'count'}).reset_index()\n",
    "x.rename(columns={'D365_ItemNo': 'Order lines', 'Order Number': 'Sales orders'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a CSV for the rest of the hours, that is, excluding the 13th and 20th hours.  This set can be used to do preparation testing.\n",
    "mask = ((df5['BaseHour'] != 12) & (df5['BaseHour'] != 19))\n",
    "df5_1 = df5[mask]\n",
    "df5_1.to_csv('output/15Dec_D365_orders_excluding_12h_19h.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak_settlement_hour.groupby(['Source Channel']).agg({'Order Number': 'nunique','D365_ItemNo': 'count'}).reset_index()\n",
    "y.rename(columns={'D365_ItemNo': 'Order lines', 'Order Number': 'Sales orders'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_count = rec_count.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales order volumes for peak order hour (12h00 to 13h00):\n",
      "  Source Channel  Sales orders  Order lines\n",
      "0            B2B           505        19601\n",
      "1            SFA          1268        20008\n",
      "2       Voice in           215         3423\n",
      "\n",
      "\n",
      "Sales order volumes for peak settlement hour (19h00 to 20h00):\n",
      "  Source Channel  Sales orders  Order lines\n",
      "0            B2B             2           42\n",
      "1            SFA            72          926\n",
      "2       Voice in             9          190\n",
      "\n",
      "\n",
      "Sales order volumes data staging in F&O:\n",
      "  Activity type Hour Sales orders Order lines\n",
      "0         2PLAN   12          462        3507\n",
      "1         2PLAN   19          566        5080\n",
      "2     3DESPATCH   12          447        5116\n",
      "3     3DESPATCH   19          396        4585\n",
      "4       4SETTLE   12          355        3781\n",
      "5       4SETTLE   19         3508       36687\n"
     ]
    }
   ],
   "source": [
    "print('Sales order volumes for peak order hour (12h00 to 13h00):')\n",
    "print(x)\n",
    "print('\\n\\nSales order volumes for peak settlement hour (19h00 to 20h00):')\n",
    "print(y)\n",
    "print('\\n\\nSales order volumes data staging in F&O:')\n",
    "print(rec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333/2635724467.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5_1.drop_duplicates(subset=['D365_Cust_No'],keep='first',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#From above, generate a file that contains just one record per customer, so that we can use this to verify that each customer master record works\n",
    "df5_1.drop_duplicates(subset=['D365_Cust_No'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_1.to_csv('output/15Dec_D365_single_line_per_customer_excluding_12h_19h.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file for stock journals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a journal line item per item per warehouse, with a replenishment_qty that is 100x the sum of order quantities (Cases) per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this dataframe before dropping columns not needed for order creation\n",
    "stock_journal = df5[['D365_ItemNo', 'D365_Del_Loc','Cases']]\n",
    "stock_journal = stock_journal.groupby(['D365_Del_Loc', 'D365_ItemNo'],as_index=False).sum('Cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal['replenishment_qty'] = stock_journal['Cases']*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with warehouse dataframe to get Financial Dimensions per warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal['INVENTORYSTATUSID'] = 'Available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1 = pd.merge(\n",
    "    stock_journal,\n",
    "    warehouses,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on='D365_Del_Loc',\n",
    "    right_on='D365_Del_Loc',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1.rename(columns={'Financial_Dimension': 'DEFAULTLEDGERDIMENSIONDISPLAYVALUE'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1.drop(columns={'Cases', 'D365_WH_NAME', 'SAP_WH_NAME','Cost_Centre'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1 = stock_journal1.sort_values(['D365_WH_NO'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a sequential index for LineNumber\n",
    "line_number = range(1,stock_journal1.last_valid_index()+2,1)\n",
    "stock_journal1['LINENUMBER']=line_number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a journal number per warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_number = pd.DataFrame(stock_journal1['D365_WH_NO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_number.rename(columns={0: 'D365_WH_NO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_number = journal_number.sort_values(['D365_WH_NO'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_valid_jnumber = input('Enter first valid journal number (numbers only)')\n",
    "#first_valid_jnumber = '76517'\n",
    "first_valid_jnumber = int(first_valid_jnumber)\n",
    "#ZA10700076636 - 24 Jan 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(first_valid_jnumber,first_valid_jnumber+journal_number.last_valid_index()+1,1)\n",
    "journal_number['JOURNALNUMBER']=index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_number['JOURNALNUMBER'] = journal_number['JOURNALNUMBER'].astype(str)\n",
    "journal_number['JOURNALNUMBER'] = 'ZA107000' +  + journal_number['JOURNALNUMBER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1 = pd.merge(\n",
    "   stock_journal1,\n",
    "   journal_number,\n",
    "   how=\"inner\",\n",
    "   on=None,\n",
    "   left_on='D365_WH_NO',\n",
    "   right_on='D365_WH_NO',\n",
    "   left_index=False,\n",
    "   right_index=False,\n",
    "   sort=True,\n",
    "   suffixes=(\"_x\", \"_y\"),\n",
    "   copy=True,\n",
    "   indicator=False,\n",
    "   validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1['ITEMBATCHNUMBER']='1'\n",
    "stock_journal1['JOURNALNAMEID']='ADJ_WHS'\n",
    "stock_journal1['TRANSACTIONDATE']= pd.to_datetime('today').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_journal1.to_csv('output/stock_journal.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
